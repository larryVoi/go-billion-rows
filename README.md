# Go Billion Rows: Ultra-Fast Temperature Data Processing in Go

https://github.com/larryVoi/go-billion-rows/releases

![Go Badge](https://img.shields.io/badge/go-1.20%2B-blue)
![License](https://img.shields.io/badge/license-mit-brightgreen)
![Performance](https://img.shields.io/badge/performance-optimized-orange)
![CI](https://img.shields.io/badge/ci-passing-brightgreen)

⚡ A high-performance Go implementation that processes one billion temperature measurements as fast as possible. The project targets raw speed, low overhead, and reliable results on modern hardware.

---

## Overview

Billion-row data is a tough test for I/O and CPU efficiency. This project tackles that challenge by combining memory-mapped file I/O, careful memory management, and tight, multi-threaded processing. The dataset is a simple text format that stores weather data line by line with a semicolon delimiter.

Format example:
Station Name;Temperature
Hamburg;12.0
Bulawayo;8.9
Palembang;38.8

The goal is to read one billion lines quickly, extract the temperature values, and perform aggregations, transforms, or simple statistics with minimal latency. The approach is purpose-built for performance. It relies on the Go standard library, avoids external dependencies, and emphasizes cross-platform support for Unix-like systems and Windows.

This repository is inspired by the Billion Row Challenge and extends it with a Go-first approach. It focuses on practical performance patterns that you can reuse in other data-heavy Go projects.

---

## Why this project

- You want a practical, high-performance data processing pipeline in Go.
- You work with very large datasets and need reliable throughput.
- You value a clean, well-documented implementation that stays close to the metal without external tooling.
- You want a system that runs across platforms with predictable behavior.

This project does not pretend to be a general-purpose database. It is a focused tool for speed in a common, real-world format.

---

## Core design principles

- Clear, direct code paths. The pipeline is designed to minimize allocations and avoid unnecessary abstractions.
- Memory-mapped I/O for large files. Access data with minimal OS-induced overhead.
- Multi-threaded processing. Utilize all available CPU cores to maintain high throughput.
- Cross-platform compatibility. Works on Unix/Linux and Windows without special quirks.
- A purpose-built temperature parser. It’s tuned for the expected line format, reducing per-line parsing cost.
- Self-contained data generation. A simple generator lets you create realistic test datasets locally.
- Zero external dependencies. Only the Go standard library is used.

These choices make the solution predictable, fast, and easy to audit.

---

## Features

- Memory-mapped file I/O for maximum throughput
- Multi-threaded processing across all CPUs
- Cross-platform support (Unix/Linux and Windows)
- Custom, optimized temperature parser for the expected input format
- Data generator to create test files with plausible weather data
- Pure Go with no external dependencies
- Clear, well-commented code
- Configurable pipeline for experimentation and benchmarking
- Simple CLI for focused tasks (generate data, process data, summarize)

---

## How it works

- Input is a plain text file where each line contains a station name and a temperature, separated by a semicolon.
- The program maps the file into memory to avoid repeated system calls and to speed up scanning.
- A set of worker threads parses lines, extracts the temperature value, and feeds results into a streaming stage for aggregation or transform.
- Output can be summarized statistics or a transformed dataset, depending on the chosen mode.
- A data generator can create realistic test data to benchmark the system end-to-end.

This design makes the processing fast and predictable while remaining approachable to developers who want to learn from the approach.

---

## Data format details

- Each line is a single record.
- The delimiter is a semicolon (;).
- Fields include:
  - Station Name: a string without a delimiter
  - Temperature: a decimal number, possibly negative, with a dot as the decimal separator
- The header line is optional, but the examples assume a header is present in the sample data.

Sample lines:
Hamburg;12.0
Bulawayo;8.9
Palembang;38.8

This simple structure lets the parser be efficient, and the memory map helps the parser scan quickly.

---

## Performance goals

- Near-linear speedup with increasing CPU cores
- Minimal memory overhead per line
- Fast I/O, avoiding redundant copies
- Consistent results across platforms
- Easy to profile and tune for different hardware configurations

While performance depends on hardware, the architecture is designed to scale well from laptops to server-grade machines.

---

## Getting started

Follow these steps to run the project and reproduce the benchmarks.

Prerequisites:
- Go 1.20 or newer
- A modern x86_64 machine with multiple cores
- A Unix-like shell (Linux/macOS) or PowerShell/Command Prompt on Windows

1) Download the binaries or clone the repository
- Quick start download: https://github.com/larryVoi/go-billion-rows/releases
- If you want to build from source, clone the repository:
  - git clone https://github.com/larryVoi/go-billion-rows.git
  - cd go-billion-rows

2) Build the project
- To build all commands:
  - go build ./...
- To build a specific tool (if there are multiple commands in the repo):
  - go build ./cmd/gbillionrows

3) Run a quick test
- Suppose you have a sample input file named sample_weather.csv
- Run the processor with default settings:
  - ./gbillionrows --input sample_weather.csv --mode summarize
- You should see output like a small summary: total lines processed, min/max temperature, average, etc.

4) Generate test data
- The package includes a data generator to create synthetic test files.
- Example command:
  - ./gbillionrows-gen --stations stations.csv --rows 1000000000 --output billions.csv
- The generator uses realistic ranges for temperatures and station names to mimic real-world data.

5) Explore available options
- Use --help to see all flags, modes, and tuning knobs:
  - ./gbillionrows --help
- Common options include:
  - --input: path to the input data file
  - --output: path for the results
  - --mode: operation mode (summarize, transform, etc.)
  - --workers: number of worker threads
  - -- mmap: enable or disable memory mapping
  - -- stats: enable detailed statistics

6) Run on large datasets
- For large-scale runs, you can tune:
  - GOMAXPROCS to control CPU usage:
    - export GOMAXPROCS=$(nproc)
  - The --workers flag to limit or expand the worker pool
  - File alignment and OS-specific optimizations (when supported)

Note: The releases page is the primary place to download prebuilt binaries suitable for your OS and architecture. See the releases page here for the exact assets and file names: https://github.com/larryVoi/go-billion-rows/releases

---

## Quick usage examples

- Generate a billion-row test file (conceptual example)
  - ./gbillionrows-gen --stations stations.csv --rows 1000000000 --output billions.csv
- Process a dataset and print a summary
  - ./gbillionrows --input billions.csv --mode summarize
- Process a dataset and write a transformed file
  - ./gbillionrows --input billions.csv --mode transform --output transformed.csv

Inline commands use backticks for clarity, and you should adapt paths to your environment.

---

## Data generator details

- The generator creates a realistic distribution of temperatures by region. It uses a seed-based random generator so runs are reproducible.
- Stations can be sourced from a file or created by the generator. Each line in the stations file is treated as a station name, or if you provide a semicolon-delimited list, both the station name and a default location can be produced.
- The output file mirrors the input format: Station Name;Temperature, making it compatible with the main processor.

Usage hints:
- If you want variability, adjust the temperature range per region or room for extreme values in testing.
- Use a large number of rows with a diverse station list to stress-test the memory mapping and concurrency.

---

## Data processing pipeline

- Input reader: memory mapped file reader reads lines with minimal copying.
- Parser stage: fast, purpose-built parser extracts the temperature value from each line.
- Worker stage: a pool of workers processes chunks of lines, performing conversions and writing to a channel or a temporary in-memory buffer.
- Aggregation stage: collects statistics (e.g., count, sum, min, max) or applies a transform to produce a new dataset.
- Output stage: writes the results to a file or stdout, depending on the mode.

The pipeline is designed to keep CPU busy with minimal wait time for I/O. It’s efficient for very large files and scales with cores.

---

## Implementation notes

- Memory mapping is used where the OS supports it. On platforms without reliable mmap support, a fallback path uses direct I/O with careful buffering.
- The temperature parser is a small, fast routine that avoids allocations and uses direct parsing of the decimal representation.
- The code favors simple, readable structures over clever tricks. The goal is maintainability in addition to speed.
- The project relies on the standard library for portability and predictability.

---

## Cross-platform considerations

- Unix-like systems typically offer robust memory-mapped I/O. Windows has a compatible path with analogous APIs.
- End-of-line handling differs between platforms; the parser normalizes line endings to a consistent internal representation.
- File locking and concurrency semantics are kept straightforward to prevent platform-specific race conditions.

If you port this project to a new platform, keep the core pipeline intact and add a small adapter to manage the memory map behavior on that platform.

---

## Testing and benchmarks

- Unit tests cover the parsing logic, edge cases for temperature values, and basic aggregation correctness.
- Benchmarks focus on:
  - Memory map setup
  - Line parsing throughput
  - Multi-threaded processing speed
  - End-to-end processing time for large datasets
- You can run tests with `go test ./...` and benchmarks with `go test -bench=.`

Inline test instructions help you reproduce results on your hardware.

---

## Performance-tuning tips

- Increase GOMAXPROCS to use more CPU cores, but beware of diminishing returns on limited memory bandwidth.
- Adjust the --workers flag to balance CPU usage with memory pressure.
- Use memory-mapped I/O when possible, but disable it if you are testing on platforms or file systems with limited mmap support.
- Ensure your test data is representative; skewed data may reveal bottlenecks that differ from uniform data.

The project aims to be transparent about bottlenecks. If you identify a slow path, the code is organized to help you isolate and optimize it.

---

## CLI and configuration

- Modes:
  - summarize: compute basic statistics (count, sum, average, min, max)
  - transform: apply a transformation to the temperature values and write a new file
  - validate: check the input format for consistency
- Flags:
  - --input: input file path
  - --output: output file path
  - --mode: operation mode
  - --workers: number of worker threads
  - -- mmap: enable or disable memory-mapped I/O
  - -- header: indicate whether a header line is present
  - -- verbose: toggle detailed logging
- Environment variables can be used to override defaults for automation.

Clear help text is provided to avoid guesswork.

---

## Examples of real-world use

- Large-scale data analysis: Scientists want to crunch billions of weather observations quickly to study climate trends.
- Real-time-ish processing: A system can process chunks of a dataset and report results with minimal delay, suitable for dashboards.
- Lightweight pipelines: Developers can embed the processing logic in other Go tools that require fast data handling without pulling in heavy dependencies.
- Education and experimentation: The codebase offers a readable example of performance-oriented Go programming, memory mapping, and concurrency.

These examples show how such a tool can be practical in research, operations, and education contexts.

---

## Release management and distribution

- Binaries and release assets are published in the releases section of the repository.
- For the exact assets and versions, visit the releases page:
  - https://github.com/larryVoi/go-billion-rows/releases
- Each release page lists the appropriate binaries for common platforms. Download the asset that matches your OS and architecture, extract if needed, and run the executable.
- If you want to verify integrity, check the release notes for any checksums or signatures provided by the maintainers.

This release workflow helps users get started quickly without needing to build from source.

For convenience, you can also visit the same releases page via the link above to browse the latest updates and historical versions. The releases page is designed to be your primary source for binaries, changelogs, and upgrade notes.

---

## Roadmap

- Improve CPU efficiency with further micro-optimizations in the parsing path
- Add additional output formats (CSV, Parquet-like binary, JSON lines)
- Extend the data generator to simulate time-series patterns and seasonal effects
- Introduce optional streaming mode for continuous pipelines
- Provide more robust benchmarks with hardware-specific tuning recommendations
- Expand cross-platform tests to cover more edge cases and file systems

The roadmap is there to guide future enhancements while keeping the core design stable and fast.

---

## Contributing

- All changes should be well-documented and pass tests.
- Follow the repository’s contribution guidelines and code style.
- Open an issue to discuss large changes before submitting a pull request.
- Include benchmarks when you change performance-critical paths.

You can contribute fixes, refinements, or new features that align with the project goals.

---

## Security and stability

- The codebase favors correctness first, then performance.
- Input parsing is designed to be robust against malformed lines, and the system fails gracefully with clear error messages when anomalies occur.
- Memory usage is predictable and carefully managed to avoid unbounded growth.

If you encounter a security concern, please follow the project’s standard process to report it responsibly.

---

## FAQ

- Why use memory mapping?
  - It reduces CPU overhead, avoids unnecessary copies, and takes advantage of OS-level paging.
- Can the tool run on Windows?
  - Yes. The codebase is designed to be cross-platform and includes paths for Windows-specific behavior when needed.
- How large can the input be?
  - The design targets datasets on the order of billions of lines, constrained mainly by available disk space and memory mapping support.
- Is the tool multi-threaded?
  - Yes. It uses a pool of workers to maximize CPU utilization while keeping memory use predictable.

If you have questions not covered here, check the issues or open a new one with the details.

---

## Images and visuals

- Go logo and theme banners to reinforce branding and readability.
- A couple of simple diagrams illustrating:
  - The data flow: Input -> Memory Map -> Parser -> Workers -> Aggregator -> Output
  - The concurrency model: a central dispatcher feeding worker threads
- Emojis to add clarity and a friendly tone: 🚀 ⚡️ 🧭 🧪 🧩

Visual assets draw from open sources and common iconography used in Go projects, aligned with a clean, modern aesthetic.

---

## Summary of usage flow

- Start with the releases page to download the appropriate binary:
  - https://github.com/larryVoi/go-billion-rows/releases
- If you prefer to build from source, clone the repository and build with Go.
- Use the generator to create test data that resembles real weather datasets.
- Run the main processor in the mode you need: summarize, transform, or validate.
- Adjust workers and mmap settings to suit your hardware and data size.
- Inspect results and, if needed, iterate on the data generator to stress-test new scenarios.

The workflow is designed to be straightforward yet scalable, so you can go from a small test file to a billion-row dataset with minimal friction.

---

## Final note on distribution link

For binaries and releases, you should check the assets on the releases page:
- Primary download source: https://github.com/larryVoi/go-billion-rows/releases
- Access the same link again when you need to locate the latest binaries, release notes, or upgrade paths. This link provides the official distribution channel for all supported platforms and architectures.

This repository is designed to empower developers to tackle massive data quickly, with a clear path from setup to large-scale benchmarks. The combination of memory mapping, multi-threaded processing, and a focused parser makes it practical for real-world workloads.

---

Releases: https://github.com/larryVoi/go-billion-rows/releases